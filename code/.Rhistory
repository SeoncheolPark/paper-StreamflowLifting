########################################
##Load Packages
########################################
Sys.setenv(TZ='Asia/Seoul')
#install.packages("https://www.lsce.ipsl.fr/Phocea/file.php?class=pisp&file=philippe.naveau/files/109/ClusterMax_1.0.tar.gz", repos=NULL, type="source")
wants <- c("abind", "adegenet", "adlift", "animation", "ape", "automap", "aws", "cluster", "ClusterMax", "CompRandFld", "DescTools", "diagram", "doParallel", "eva", "extRemes", "fclust", "fda", "fields", "foreach", "forecast", "geoR", "ggmap", "ggplot2", "glmnet", "gridExtra", "gstat", "igraph", "lmridge", "lubridate", "mapdata", "maps", "maptools", "nlt" , "nnls", "nsprcomp", "POT", "psych", "quantreg", "quantreg.nonpar", "rgdal", "rgeos", "rospca", "rrcov", "shape", "shapefiles", "sp", "SpatialExtremes", "stringr", "texmex", "TSA", "timeDate", "tseries", "wavethresh", "xts")
has   <- wants %in% rownames(installed.packages())
if(any(!has)) install.packages(wants[!has])
lapply(wants, library, character.only=T)
PM10 <- readRDS("~/Dropbox/Data/PM/FromKMA/KoreaPM.RDS")
PM10$data <- PM10$data["2008-07-01/",]
##Only focused on Sudogwon area
PM10$data <- PM10$data[,c(3,4,6,7,11,14)]
PM10$place <- PM10$place[c(3,4,6,7,11,14),]
#PM10$shape <- subset(PM10$shape, PM10$shape@data$CTP_ENG_NM=="Seoul" | PM10$shape@data$CTP_ENG_NM=="Gyeonggi-do" | PM10$shape@data$CTP_ENG_NM=="Incheon" | PM10$shape@data$CTP_ENG_NM=="Gangwon-do" | PM10$shape@data$CTP_ENG_NM=="Chungcheongnam-do" | PM10$shape@data$CTP_ENG_NM=="Chungcheongbuk-do" | PM10$shape@data$CTP_ENG_NM=="Sejong-si" | PM10$shape@data$CTP_ENG_NM=="Daejeon" | PM10$shape@data$CTP_ENG_NM=="Daegu" | PM10$shape@data$CTP_ENG_NM=="Gyeongsangbuk-do")
PM10$shape <- subset(PM10$shape, PM10$shape@data$CTP_ENG_NM=="Seoul" | PM10$shape@data$CTP_ENG_NM=="Gyeonggi-do" | PM10$shape@data$CTP_ENG_NM=="Incheon")
PM10_AirKorea <- readRDS("~/Dropbox/Data/PM/From2001/KoreaPM10.RDS")
PM10_AirKorea$data <- PM10_AirKorea$data[,which(PM10_AirKorea$place$시도=="서울" | PM10_AirKorea$place$시도=="인천" | PM10_AirKorea$place$시도=="경기")]
PM10_AirKorea$place <- PM10_AirKorea$place[which(PM10_AirKorea$place$시도=="서울" | PM10_AirKorea$place$시도=="인천" | PM10_AirKorea$place$시도=="경기"),]
PM10_AirKorea$data <- PM10_AirKorea$data["2008-07-01/2019-06-30",]
#PM10_AirKorea$data <- PM10_AirKorea$data["2008-07-01 01:00:00/2019-07-01 00:00:00",]
#climate data와 PM10 data 순서 맞추기
EN_names <- c("Baengnyeongdo", "Seoul", "Gwanaksan", "Suwon", "Ganghwa", "Yeonpyeongdo")
#EN_names <- c("Daegwallyeong", "Chuncheon", "Baengnyeong-do", "Seoul", "Ulleung-do", "Gwanaksan", "Suwon", "Youngwol",
#              "Cheonan", "Anmyeon-do", "Andong", "Ganghwa", "Geokryeolbiyeoldo", "Gwangdeoksan", "Sokcho")
climate_data_ASOS <- readRDS("~/Dropbox/Data/Climate(Korea)/Hourly(ASOS)_Sudogwon/KoreaWeahter(ASOS).RDS")
climate_data_ASOS$place <- rbind(climate_data_ASOS$place[-c(1,2),], climate_data_ASOS$place[c(1,2),])
#cbind(climate_data_ASOS$place$지점주소, colnames(climate_data_ASOS$precip))
climate_data_AWS <- readRDS("~/Dropbox/Data/Climate(Korea)/Hourly(AWS)_Sudogwon/KoreaWeahter(AWS).RDS")
#cbind(climate_data_AWS$place$지점주소, colnames(climate_data_AWS$precip))
plot(PM10$shape)
points(PM10$place$경도, PM10$place$위도, col=2, pch=16, cex=2)
points(PM10_AirKorea$place$경도, PM10_AirKorea$place$위도, col=7, pch=15)
points(climate_data_ASOS$place$경도, climate_data_ASOS$place$위도, col=4, pch=0, cex=1)
points(climate_data_AWS$place$경도, climate_data_AWS$place$위도, col=3, pch=0, cex=1)
legend("bottomleft", c("PM10", "PM10(AirKorea)", "ASOS", "AWS"), pch=c(16,15,0,0), col=c(2,7,4,3))
#summary
apply(as.matrix(PM10$data), 2, function(x) sum(is.na(x)))/nrow(PM10$data)
apply(as.matrix(PM10$data), 2, function(x) summary(as.numeric(x)))
#분석에 쓸 station들만 남겨놓자
#(102) 백령도 (108) 서울 (119) 수원 (201) 강화
PM10$data <- PM10$data[,c(1,2,5)]
PM10$place <- PM10$place[c(1,2,5),]
precip_data <- climate_data_ASOS$precip["2008-07-01/",c(1,2,5)]
temp_data <- climate_data_ASOS$temp["2008-07-01/",c(1,2,5)]
winddirection_data <- climate_data_ASOS$winddirection["2008-07-01/",c(1,2,5)]
windspeed_data <- climate_data_ASOS$windspeed["2008-07-01/",c(1,2,5)]
########################################
##Missing value 채우기 : WARNING (TIME CONSUMING)
########################################
#according to AirKorea site
#백령도(102) <- 백령도 (수도권 자료 PM10_AirKorea 137번, 2012년부터 있음)
#서울(108) <- 중구 (수도권 자료 PM10_AirKorea 1번)
#강화(201) <- 인천 송해 (수도권 자료 PM10_AirKorea 68번)
#[1] 0.9265047
cor(as.numeric(PM10$data$`102`), as.numeric(PM10_AirKorea$data$백령도), use="complete.obs")
#[1] 0.9551483
cor(as.numeric(PM10$data$`108`), as.numeric(PM10_AirKorea$data$중구), use="complete.obs")
#[1] 0.8643007
cor(as.numeric(PM10$data$`201`), as.numeric(PM10_AirKorea$data$송해), use="complete.obs")
PM10_imputed <- PM10$data
for(i in 1:ncol(PM10_imputed)){
if(i==1){
j=137
}else if(i==2){
j=1
}else if(i==3){
j=68
}
i_candidate <- which(is.na(PM10_imputed[,i]))
j_candidate <- which(!is.na(PM10_AirKorea$data[,j]))
length(intersect(i_candidate, j_candidate))/length(i_candidate)
}
########################################
##Marginal distribution modeling
#(1) kernel density estimation
########################################
PM10_spring <- subset(PM10$data, lubridate::month(PM10$data)%in%c(3,4,5))
precip_spring <- subset(precip_data, lubridate::month(precip_data)%in%c(3,4,5))
temp_spring <- subset(temp_data, lubridate::month(temp_data)%in%c(3,4,5))
winddirection_spring <- subset(winddirection_data, lubridate::month(winddirection_data)%in%c(3,4,5))
windspeed_spring <- subset(windspeed_data, lubridate::month(windspeed_data)%in%c(3,4,5))
par(mfrow=c(2,2))
plot(as.numeric(precip_spring$`108`), as.numeric(PM10_spring$`108`), main="(a) PM10 and Precip")
plot(as.numeric(temp_spring$`108`), as.numeric(PM10_spring$`108`), main="(b) PM10 and Temp")
plot(as.numeric(winddirection_spring$`108`), as.numeric(PM10_spring$`108`), main="(c) PM10 and Winddirection")
plot(as.numeric(windspeed_spring$`108`), as.numeric(PM10_spring$`108`), main="(d) PM10 and Windspeed")
par(mfrow=c(3,1))
hist(PM10_spring$`102`)
hist(PM10_spring$`108`)
hist(PM10_spring$`201`)
kerneldensity_B <- density(PM10_spring$`102`, na.rm=TRUE)
kerneldensity_S <- density(PM10_spring$`108`, na.rm=TRUE)
kerneldensity_G <- density(PM10_spring$`201`, na.rm=TRUE)
#kernel density estimation result
par(mfrow=c(3,1))
plot(kerneldensity_B, main="(a) Baeknyeong", xlim=c(0,850), ylim=c(0,0.02))
plot(kerneldensity_G, main="(b) Ganghwa", xlim=c(0,850), ylim=c(0,0.02))
plot(kerneldensity_S, main="(c) Seoul", xlim=c(0,850), ylim=c(0,0.02))
########################################
##Marginal distribution modeling
#(2) GPD estimation result
########################################
#Q. how to select a threhold?
#95% empirical quantile (in Ph.D. Thesis of Huser)
#using extremalIndexRangeFit in texmex
erf_B <- extremalIndexRangeFit(PM10_spring$`102`[complete.cases(data=PM10_spring$`102`)], umin=75,umax=120,verb=FALSE)
erf_S <- extremalIndexRangeFit(PM10_spring$`108`[complete.cases(data=PM10_spring$`108`)], umin=75,umax=120,verb=FALSE)
erf_G <- extremalIndexRangeFit(PM10_spring$`201`[complete.cases(data=PM10_spring$`201`)], umin=75,umax=120,verb=FALSE)
u <- apply(PM10_spring, 2, function(x) quantile(x, probs=0.9, na.rm=T))
gpdFit_B <- gpdFit(data=PM10_spring$`102`[complete.cases(data=PM10_spring$`102`)], threshold=u[1])
gpdFit_S <- gpdFit(data=PM10_spring$`108`[complete.cases(data=PM10_spring$`108`)], threshold=u[2])
gpdFit_G <- gpdFit(data=PM10_spring$`201`[complete.cases(data=PM10_spring$`201`)], threshold=u[3])
#plot(gpdFit_B)
#plot(gpdFit_S)
#plot(gpdFit_G)
x_plot <- seq(0, 800, length.out = 1000)
gpd_estimate_B <- eva::dgpd(x_plot, loc=gpdFit_B$threshold, scale=gpdFit_B$par.ests[1], shape=gpdFit_B$par.ests[2])
gpd_estimate_S <- eva::dgpd(x_plot, loc=gpdFit_S$threshold, scale=gpdFit_S$par.ests[1], shape=gpdFit_S$par.ests[2])
gpd_estimate_G <- eva::dgpd(x_plot, loc=gpdFit_G$threshold, scale=gpdFit_G$par.ests[1], shape=gpdFit_G$par.ests[2])
#plot(x_plot, gpd_estimate_B)
par(mfrow=c(3,1))
plot(kerneldensity_B, main="(a) Baeknyeong", xlim=c(0,850), ylim=c(0,0.02))
#scaled by 0.05
lines(x_plot[which(x_plot>gpdFit_B$threshold)], gpd_estimate_B[which(x_plot>gpdFit_B$threshold)]*0.1, col=4)
abline(v=80, col=2, lty=2)
plot(kerneldensity_G, main="(b) Ganghwa", xlim=c(0,850), ylim=c(0,0.02))
lines(x_plot[which(x_plot>gpdFit_S$threshold)], gpd_estimate_S[which(x_plot>gpdFit_S$threshold)]*0.1, col=4)
abline(v=80, col=2, lty=2)
plot(kerneldensity_S, main="(c) Seoul", xlim=c(0,850), ylim=c(0,0.02))
lines(x_plot[which(x_plot>gpdFit_G$threshold)], gpd_estimate_G[which(x_plot>gpdFit_G$threshold)]*0.1, col=4)
abline(v=80, col=2, lty=2)
#꼬리부분에서 잘 맞지 않지만 여기서 적합한 GPD 모형은
#declustering도 하지 않고 timewise하게 independent하다는 가정 하에
#적합했음
#print GPD estimation result
show(gpdFit_B)
show(gpdFit_S)
show(gpdFit_G)
########################################
##Marginal distribution modeling
#(3) declustering method
########################################
exI_B <- extremalIndex(y=PM10_spring$`102`[complete.cases(data=PM10_spring$`102`)], threshold=u[1])
exI_S <- extremalIndex(y=PM10_spring$`108`[complete.cases(data=PM10_spring$`108`)], threshold=u[2])
exI_G <- extremalIndex(y=PM10_spring$`201`[complete.cases(data=PM10_spring$`201`)], threshold=u[3])
par(mfrow=c(3,1))
plot(exI_B)
plot(exI_S)
plot(exI_G)
#evm with original dataset
gpd_B <- evm(y=as.numeric(PM10_spring$`102`[complete.cases(data=PM10_spring$`102`)]), th=u[1])
gpd_S <- evm(y=as.numeric(PM10_spring$`108`[complete.cases(data=PM10_spring$`108`)]), th=u[2])
gpd_G <- evm(y=as.numeric(PM10_spring$`201`[complete.cases(data=PM10_spring$`201`)]), th=u[3])
ggplot(gpd_B)
ggplot(gpd_S)
ggplot(gpd_G)
dc_B <- declust(y=PM10_spring$`102`[complete.cases(data=PM10_spring$`102`)], threshold=u[1])
dc_S <- declust(y=PM10_spring$`108`[complete.cases(data=PM10_spring$`108`)], threshold=u[2])
dc_G <- declust(y=PM10_spring$`201`[complete.cases(data=PM10_spring$`201`)], threshold=u[3])
dcgpd_B <- evm(dc_B)
dcgpd_S <- evm(dc_S)
dcgpd_G <- evm(dc_G)
#Huser의 박사학위 논문에 따르면 그냥 적합한 결과와 declustering한 후
#적합한 결과의 모수추정값들이 큰 차이가 나지 않아야 한다.
result_fitGPD <- matrix(0, nrow=2, ncol=ncol(PM10$data))
rownames(result_fitGPD) <- c("scale", "shape")
colnames(result_fitGPD) <- colnames(PM10$data)
result_fitGPD[,1] <- evd::fpot(x=PM10_spring$`102`[complete.cases(data=PM10_spring$`102`)], threshold=u[1], model="gpd")$estimate
result_fitGPD[,2] <- evd::fpot(x=PM10_spring$`108`[complete.cases(data=PM10_spring$`108`)], threshold=u[2], model="gpd")$estimate
result_fitGPD[,3] <- evd::fpot(x=PM10_spring$`201`[complete.cases(data=PM10_spring$`201`)], threshold=u[3], model="gpd")$estimate
########################################
##Marginal distribution modeling
#(43-1) extremal index computation
#according to chapter 7, Dey (2016), 1/theta is the
#limiting mean size of clusters of exceedances of the threshold u
########################################
########################################
##Marginal distribution modeling
#(4) Stationary check
########################################
#temporal stationary check (Ph.D. thesis of Raphael Huser)
#연도별로 나눠서 GPD  fitting
#2008 2009   2010        2011        2012(윤년)              2013              2014              2015              2016(윤년)              2017              2018
year_index_indicator <- c(0, 365*24, (365*2)*24, (365*3)*24, (365*3+366*1)*24, (365*4+366*1)*24, (365*5+366*1)*24, (365*6+366*1)*24, (365*6+366*2)*24, (365*7+366*2)*24, (365*8+366*2)*24)
result_fitGPD_yearly <- array(0, dim=c(2,ncol(PM10$data), length(year_index_indicator)), dimnames=list(c("scale","shape"), colnames(PM10$data), c(2008:2018)))
date_linear <- c(1:nrow(PM10$data))/nrow(PM10$data)
for(i in 1:ncol(PM10$data)){
for(j in 1:length(year_index_indicator)){
#create design matrix
design_matrix <- data.frame(PM10=as.numeric(PM10$data[,i]), linear=date_linear)
if(j==length(year_index_indicator)){
year_index <- c((year_index_indicator[j]+1):nrow(PM10$data))
}else{
year_index <- c((year_index_indicator[j]+1):year_index_indicator[(j+1)])
}
design_matrix <- design_matrix[year_index,]
date_linear_year <- date_linear[year_index]
est_threshold <- rq(PM10 ~ linear, data=design_matrix, tau=0.9)
PM10data_check <- design_matrix$PM10[complete.cases(design_matrix)] #- est_threshold$fitted.values
PM10data_check <- PM10data_check[PM10data_check>0]
model<- extRemes::fevd(PM10data_check,threshold = u[i],type="GP") #extRemes package
#model
model2 <- fExtremes::gpdFit(PM10data_check, u=u[i], type="mle", doplot=T)
#model2
model3 <- evd::fpot(x=PM10data_check, threshold=u[i], model="gpd")
#model3
result_fitGPD_yearly[,i,j] <- model3$estimate
}
}
result_fitGPD
result_fitGPD_yearly
#shape 모수들이 음수인 곳: 2011년, 2013년
n_block_iter <- 100
#block bootstrap을 해보자
result_fitGPD_yearly_block <- array(0, dim=c(2,ncol(PM10$data), length(year_index_indicator),n_block_iter ), dimnames=list(c("scale","shape"), colnames(PM10$data), c(2008:2018), c(1:n_block_iter)))
for(i in 1:ncol(PM10$data)){
for(j in 1:length(year_index_indicator)){
for(k in 1:n_block_iter){
#create design matrix
design_matrix <- data.frame(PM10=as.numeric(PM10$data[,i]), linear=date_linear)#,sine=date_sine,cosine=date_cosine)
if(j==length(year_index_indicator)){
year_index <- c((year_index_indicator[j]+1):nrow(PM10$data))
}else{
year_index <- c((year_index_indicator[j]+1):year_index_indicator[(j+1)])
}
design_matrix <- design_matrix[year_index,]
date_linear_year <- date_linear[year_index]
est_threshold <- rq(PM10 ~ linear, data=design_matrix, tau=0.9)
resample_index <- year_block_estimation(data=design_matrix$PM10)
new_index <- match(resample_index, which(complete.cases(design_matrix)))
PM10data_check <- design_matrix$PM10[new_index] #- est_threshold$fitted.values[new_index]
PM10data_check <- PM10data_check[complete.cases(PM10data_check)]
PM10data_check <- PM10data_check[PM10data_check>0]
model<-fevd(PM10data_check,threshold = u[i],type="GP")
#model
model2 <- fExtremes::gpdFit(PM10data_check, u=u[i], type="mle", doplot=T)
#model2
model3 <- evd::fpot(x=PM10data_check, threshold=u[i], model="gpd")
#model3
result_fitGPD_yearly_block[,i,j,k] <- model3$estimate
}
}
}
#plotting the result
boot_estimation <- list()
scale_estimation_mat = shape_estimation_mat <- array(0, dim=c(3, ncol(PM10$data), length(year_index_indicator)), dimnames = list(c("mean", "lower", "upper"), colnames(PM10$data), c(2008:2018)))
for(i in 1:ncol(PM10$data)){
#i: 장소
for(j in 1:dim(result_fitGPD_yearly_block)[3]){
#j: 연도
scale_estimation_mat[1,i,j] <- result_fitGPD_yearly[1,i,j]
scale_estimation_mat[c(2:3),i,j] <- sort(result_fitGPD_yearly_block[1,i,j,])[c(5,95)]
shape_estimation_mat[1,i,j] <- result_fitGPD_yearly[2,i,j]
shape_estimation_mat[c(2:3),i,j] <- sort(result_fitGPD_yearly_block[2,i,j,])[c(5,95)]
}
}
#이것을 연도별로 MLE 추정한 추정값을 보여준다
par(mfrow=c(1,2))
plot_year_index <- c(2008:2018)
for(i in 1:ncol(PM10$data)){
#i: 장소
#for scale parameter
plot(plot_year_index,scale_estimation_mat[1,i,], ylim=range(scale_estimation_mat[,i,]), type="n", xlab="", ylab="", main=paste("(a) Scale, ", colnames(PM10$data)[i]) )
abline(h=result_fitGPD[1,i], col=2, lwd=2)
abline(h=result_fitGPD[1,i]-5, col="grey")
abline(h=result_fitGPD[1,i]+5, col="grey")
points(plot_year_index, result_fitGPD_yearly[1,i,], pch=19)
for(j in 1:length(plot_year_index)){
segments(x0=plot_year_index[j], y0=scale_estimation_mat[2,i,j], y1=scale_estimation_mat[3,i,j])
}
#for shape parameter
plot(plot_year_index,shape_estimation_mat[1,i,], ylim=range(shape_estimation_mat[,i,]), type="n", xlab="", ylab="", main=paste("(b) shape, ", colnames(PM10$data)[i]) )
abline(h=result_fitGPD[2,i], col=2, lwd=2)
abline(h=result_fitGPD[2,i]-0.3, col="grey")
abline(h=result_fitGPD[2,i]+0.3, col="grey")
points(plot_year_index, result_fitGPD_yearly[2,i,], pch=19)
for(j in 1:length(plot_year_index)){
segments(x0=plot_year_index[j], y0=shape_estimation_mat[2,i,j], y1=shape_estimation_mat[3,i,j])
}
}
########################################
##Unit Frechet transform
########################################
newdata_unitfrechet <- PM10$data
for(i in 1:ncol(PM10$data)){
design_matrix <- data.frame(PM10=as.numeric(PM10$data[,i]), linear=date_linear)
year_index <- c(1:nrow(PM10$data))
design_matrix <- design_matrix[year_index,]
date_linear_year <- date_linear[year_index]
#est_threshold <- rq(PM10 ~ linear, data=design_matrix, tau=0.9)
#predict_est_threshold <- predict(est_threshold, newdata=design_matrix[,c(2:4)])
#z <- pgpd(design_matrix$PM10, loc=predict_est_threshold, scale=result_fitGPD[1,i], shape=result_fitGPD[2,i])
#(MAR 10, 2020)
#아래 코드의 unit frechet fransform 결과가 이상해서
#POT 패키지의 gpd2frech 함수를 사용
#z <- evd::pgpd(design_matrix$PM10, loc=u[i], scale=result_fitGPD[1,i], shape=result_fitGPD[2,i])
#z <- -1/log(1-pat*z)
z <- gpd2frech(design_matrix$PM10, loc=u[i], scale=result_fitGPD[1,i], shape=result_fitGPD[2,i])
newdata_unitfrechet[,i] <- z
}
newdata_unitfrechet <- as.matrix(newdata_unitfrechet)
newdata_unitfrechet[newdata_unitfrechet==-Inf] <- NA
newdata_unitfrechet_list <- list()
for(i in 1:ncol(PM10$data)){
newdata_unitfrechet_list[[i]] <- newdata_unitfrechet[(newdata_unitfrechet[,i]>=0 & complete.cases(newdata_unitfrechet[,i]) ) ,i]
plot(newdata_unitfrechet_list[[i]])
}
########################################
##Huser 박사학위 논문 200쪽
##Another concern is the presence of temporal dependence that may affect the estimation of marginal parameters.
##Extremal index using the intervals estimator (1.21) proposed by Ferro & Segers (2003)
##관련: extRemes 패키지 decluster 함수
##POT 함수에도 declustering이 들어있다 https://cran.r-project.org/web/packages/POT/vignettes/POT.pdf
##Decluster data above a given threshold to try to make them independent.
##declustering의 목적: to remove temporal dependence in the (time) series (Huser 박사학위 논문 28쪽)
##coles책 5.3.2에도 관련 설명 있음
########################################
#marginal estimate는 declustering scheme에 꽤 민감한 것으로 알려져, Schlather model with beta random sets에 대해 temporal max-stable model을 적합시킨다
library(extRemes)
ii <- 1
newdata_detrended_forcluster <- data.frame(obs=as.numeric(PM10$data[,ii]), time=index(PM10$data))
newdata_detrended_forcluster <- newdata_detrended_forcluster[complete.cases(newdata_detrended_forcluster),]
#Usually, we want only cluster maxima, this is achieved by passing option clust.max = TRUE.
clust(newdata_detrended_forcluster, u=0, clust.max = TRUE, tim.comd=8) #exi: giving an estimation of the Extremal Index
########################################
##Marginal distribution modeling
#(5) Spatio-Temporla dependence
#extremal coeff 출력
##`SpatialExtremes`
##패키지 안의 fitextcoeff 함수 활용
##censored Schlather–Tawn estimator를 쓴다고 한다
########################################
#spatial version
fitextcoeff(data=na.omit(newdata_unitfrechet), coord = cbind(PM10$place$경도, PM10$place$위도), estim="ST", marge="frech") #1에 가까움: 엄청 dependent한 상황
#fitextcoeff(data=na.omit(newdata_spring_unitfrechet[,c(1,2)]), coord = cbind(newplace$경도, newplace$위도)[c(1,2)], estim="ST", marge="frech")
fitextcoeff(data=na.omit(as.matrix(PM10$data)), coord = cbind(PM10$place$경도, PM10$place$위도), estim="ST", marge="emp", prob=0.9) #1에 가까움: 엄청 dependent한 상황
library(Hmisc) #밑에 나오는 Lag 함수 사용 위함
#places: 2(백령도) 8(강화) 3(서울) 5(영월)
sub_index <- c(1,2,3)
result_fitextcoeff <- c()
#(MAR 10, 2020)
#data change
newdata_unitfrechet <- as.matrix(PM10$data)
for(ii in 1:length(sub_index)){
for(jj in 1:length(sub_index)){
data_test <- cbind(newdata_unitfrechet[,sub_index[jj]],newdata_unitfrechet[,sub_index[ii]])
##
#one option
len <- 12
fibvals <- numeric(len)
fibvals[1] <- 1
fibvals[2] <- 1
for (i in 3:len) {
fibvals[i] <- fibvals[i-1]+fibvals[i-2]
}
##
length_lag <- fibvals[c(2:12)]
length_lag <- c(1:75)
data_test_coord <- matrix(0, nrow=(length(length_lag)+2), ncol=1)
for(i in 1:length(length_lag)){
#Lag: function in quantmod library
data_test <- cbind(data_test, Lag(newdata_unitfrechet[,sub_index[ii]], +length_lag[i]))
data_test_coord[(i+2)] <- length_lag[i]
}
result_fitextcoeff_imsi <- fitextcoeff(data=data_test, coord=data_test_coord, prob=0.9, marge="emp")
#result_fitextcoeff_imsi <- fitextcoeff(data=data_test, coord=data_test_coord)
result_fitextcoeff <- cbind(result_fitextcoeff, result_fitextcoeff_imsi$ext.coeff[c(1:(length(length_lag)+1)),2])
}
}
#library(extrafont)
#font_import()
par(mfrow=c(length(sub_index),length(sub_index)))
par(mar=c(4.1,4.1,2.1,2.1)/2)
length_lag_with0 <- c(0,length_lag)
for(ii in 1:length(sub_index)){
for(jj in 1:length(sub_index)){
plot(length_lag_with0, result_fitextcoeff[,(jj+length(sub_index)*(ii-1))], type='o', xlim=c(0,max(length_lag)), ylim=c(1,2), ylab = '', xlab = '', main=paste(PM10$place$지점[sub_index[ii]],",",paste(PM10$place$지점[sub_index[jj]])))
abline(v=length_lag_with0[which.min(result_fitextcoeff[,(jj+length(sub_index)*(ii-1))])], col="red")
}
}
#일별효과 존재? (예를 들면 밤엔 낮고 낮엔 높다)
#likelihood를 어떻게 모델링할까
